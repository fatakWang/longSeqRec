{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fatak\\.conda\\envs\\longSeq\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2733)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.randn([3,1])\n",
    "tensor2 = torch.ones([3,1])\n",
    "\n",
    "tensor3 = nn.Sigmoid()(tensor1)\n",
    "nn.BCELoss()(tensor3,tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "当前处理到第10步: 100%|██████████| 10/10 [00:05<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 创建一个tqdm进度条示例，这里以简单的循环为例\n",
    "progress_bar = tqdm(range(10))\n",
    "for i in progress_bar:\n",
    "    # 根据循环情况更新描述信息\n",
    "    progress_bar.set_description(f'当前处理到第{i + 1}步')\n",
    "    # 模拟一些其他操作，这里只是简单的暂停\n",
    "    import time\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NDCG@1', 'NDCG@5', 'NDCG@10', 'Recall@1', 'Recall@5', 'Recall@10']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_ks = [1, 5, 10, 20, 50]\n",
    "\n",
    "description_metrics = ['NDCG@%d' % k for k in metric_ks[:3]\n",
    "                               ] + ['Recall@%d' % k for k in metric_ks[:3]]\n",
    "description = 'Eval: ' + \\\n",
    "    ', '.join(s + ' {:.4f}' for s in description_metrics)\n",
    "description = description.replace('NDCG', 'N').replace('Recall', 'R')\n",
    "\n",
    "description_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fatak\\.conda\\envs\\longSeq\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# x = nn.Embedding(10, 64, padding_idx=0)\n",
    "y = torch.randint(0,10,(80,100,5))\n",
    "# x(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = test()\n",
    "args.abc = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r',encoding='utf-8') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    args = DynamicClass(config)\n",
    "    return args\n",
    "class DynamicClass:\n",
    "    def __init__(self, input_dict):\n",
    "        for key, value in input_dict.items():\n",
    "            setattr(self, key, value)\n",
    "a = load_config(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'> -> 1.23e-09\n",
      "0.0456\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "yaml_data = \"\"\"\n",
    "my_number: 1.23e-9\n",
    "another_number: 4.56e-2\n",
    "\"\"\"\n",
    "\n",
    "data = yaml.safe_load(yaml_data)\n",
    "print(f\"{type(data[\"my_number\"])} -> {data[\"my_number\"]}\")  \n",
    "print(data[\"another_number\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DynamicClass(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "class DynamicClass:\n",
    "    def __init__(self, input_dict):\n",
    "        for key, value in input_dict.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    args = DynamicClass(config)\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .base import AbstractDataloader\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.utils.data as data_utils\n",
    "class SASTrainDataset(data_utils.Dataset):\n",
    "    def __init__(self, args, u2seq, max_len, sliding_size, rng,item_count):\n",
    "        self.args = args\n",
    "        self.max_len = max_len # 序列长度\n",
    "        self.sliding_step = int(sliding_size * max_len)\n",
    "        self.num_items = args.num_items\n",
    "        self.rng = rng\n",
    "        self.item_count = item_count\n",
    "        \n",
    "        assert self.sliding_step > 0\n",
    "        self.all_seqs = []\n",
    "        self.pos_item = []\n",
    "        self.neg_item = [] # 直接从seq中无放回抽滑动次数个\n",
    "        all_item_set = {i for i in range(1,self.item_count+1)} # 所有item的集合,【1-self.item_count】\n",
    "        for u in sorted(u2seq.keys()):\n",
    "            seq = u2seq[u]\n",
    "            neg_item_pool = all_item_set - set(seq)\n",
    "            # 序列长度小则，直接加\n",
    "            if len(seq) < self.max_len + self.sliding_step:\n",
    "                self.all_seqs.append(seq[:-1])\n",
    "                self.pos_item.append(seq[-1])\n",
    "                self.neg_item.append(random.sample(neg_item_pool,1)) # 在除了seq的item中随机抽一个，也不要是0\n",
    "            else:\n",
    "                # 序列长度大于则拆开，相当于是滑动窗口版本了，start_idx就是起始idx，从最后一个能有max_LEN的到最后一位。\n",
    "                start_idx = range(len(seq) - max_len -1, -1, -self.sliding_step)\n",
    "                # append是加到最后一位，+是将【】解包，然后在逐个append。\n",
    "                self.all_seqs = self.all_seqs + [seq[i:i + max_len] for i in start_idx]\n",
    "                self.pos_item = self.pos_item + [seq[i+max_len] for i in start_idx]\n",
    "                # TODO 有没有可能len(start_idx)大于了neg_item_pool，应该不可能吧\n",
    "                self.neg_item = self.neg_item + random.sample(neg_item_pool,len(start_idx))\n",
    "                print(f\"{ [seq[i:i + max_len] for i in start_idx]} {[seq[i+max_len] for i in start_idx]} {random.sample(neg_item_pool,len(start_idx))} {[i for i in start_idx]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_seqs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # index与user解绑了\n",
    "        # 还需要加上0\n",
    "        seq = self.all_seqs[index][-self.max_len:]\n",
    "        mask_len = self.max_len - len(seq)\n",
    "        seq = [0] * mask_len + seq\n",
    "        target_pos = self.pos_item[index]\n",
    "        target_neg = self.neg_item[index]\n",
    "        # print(f\"{len(seq)} {len(target_pos)} {len(target_neg)}\")\n",
    "        return torch.LongTensor(seq), torch.LongTensor(target_pos) , torch.LongTensor(target_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "class DynamicClass:\n",
    "    def __init__(self, input_dict):\n",
    "        for key, value in input_dict.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r',encoding='utf-8') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    args = DynamicClass(config)\n",
    "    return args\n",
    "\n",
    "args = load_config(\"C:\\\\D\\code\\\\longSeqRec\\\\config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DynamicClass' object has no attribute 'num_items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m train \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m item_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmap\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m \u001b[43mSASTrainDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mitem_count\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m, in \u001b[0;36mSASTrainDataset.__init__\u001b[1;34m(self, args, u2seq, max_len, sliding_size, rng, item_count)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len \u001b[38;5;241m=\u001b[39m max_len \u001b[38;5;66;03m# 序列长度\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sliding_size \u001b[38;5;241m*\u001b[39m max_len)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_items \u001b[38;5;241m=\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_items\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng \u001b[38;5;241m=\u001b[39m rng\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_count \u001b[38;5;241m=\u001b[39m item_count\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DynamicClass' object has no attribute 'num_items'"
     ]
    }
   ],
   "source": [
    "file_name = \"data\\\\preprocessed\\\\ml-1m_min_rating0-min_uc5-min_sc5-leave_one_out\\\\dataset.pkl\"\n",
    "with open(file_name, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "train = data[\"train\"]\n",
    "item_count = len(data['smap'])\n",
    "SASTrainDataset(args,train,200,1,None,item_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import random\n",
    "file_name = \"data\\\\preprocessed\\\\ml-1m_min_rating0-min_uc5-min_sc5-leave_one_out\\\\dataset.pkl\"\n",
    "with open(file_name, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "val = data[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1={1,2,3}\n",
    "set2={5}\n",
    "set1 & set2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;241m-\u001b[39m max_len \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39msliding_step)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# append是加到最后一位，+是将【】解包，然后在逐个append。\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m all_seqs \u001b[38;5;241m=\u001b[39m all_seqs \u001b[38;5;241m+\u001b[39m [seq[i:i \u001b[38;5;241m+\u001b[39m max_len] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m start_idx]\n\u001b[0;32m     29\u001b[0m pos_item \u001b[38;5;241m=\u001b[39m pos_item \u001b[38;5;241m+\u001b[39m [seq[i\u001b[38;5;241m+\u001b[39mmax_len] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m start_idx]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# TODO 有没有可能len(start_idx)大于了neg_item_pool，应该不可能吧\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# neg_item = neg_item + random.sample(neg_item_pool,len(start_idx))\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# print(f\"{ [seq[i:i + max_len] for i in start_idx]} {[seq[i+max_len] for i in start_idx]} {random.sample(neg_item_pool,len(start_idx))} {[i for i in start_idx]}\")\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import random\n",
    "file_name = \"data\\\\preprocessed\\\\ml-1m_min_rating0-min_uc5-min_sc5-leave_one_out\\\\dataset.pkl\"\n",
    "with open(file_name, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "train = data[\"train\"]\n",
    "item_count = len(data['smap'])\n",
    "sliding_step=1\n",
    "max_len=200\n",
    "all_seqs = []\n",
    "pos_item = []\n",
    "neg_item = []\n",
    "u2seq = train\n",
    "all_item_set = {i for i in range(1,item_count+1)}\n",
    "for u in sorted(u2seq.keys()):\n",
    "    seq = u2seq[u]\n",
    "    neg_item_pool = all_item_set - set(seq)\n",
    "    # 序列长度小则，直接加\n",
    "    if len(seq) < max_len + sliding_step:\n",
    "        all_seqs.append(seq[:-1])\n",
    "        pos_item.append(seq[-1])\n",
    "        # neg_item.append(random.sample(neg_item_pool,1)) # 在除了seq的item中随机抽一个，也不要是0\n",
    "    else:\n",
    "        # 序列长度大于则拆开，相当于是滑动窗口版本了，start_idx就是起始idx，从最后一个能有max_LEN的到最后一位。\n",
    "        start_idx = range(len(seq) - max_len -1, -1, -sliding_step)\n",
    "        # append是加到最后一位，+是将【】解包，然后在逐个append。\n",
    "        all_seqs = all_seqs + [seq[i:i + max_len] for i in start_idx]\n",
    "        pos_item = pos_item + [seq[i+max_len] for i in start_idx]\n",
    "        # TODO 有没有可能len(start_idx)大于了neg_item_pool，应该不可能吧\n",
    "        # neg_item = neg_item + random.sample(neg_item_pool,len(start_idx))\n",
    "        # print(f\"{ [seq[i:i + max_len] for i in start_idx]} {[seq[i+max_len] for i in start_idx]} {random.sample(neg_item_pool,len(start_idx))} {[i for i in start_idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [i for i in range(10)]\n",
    "max_len = 5\n",
    "start_idx = range(len(seq) - max_len -1, -1, -1)\n",
    "x = [seq[i:i + max_len] for i in start_idx]\n",
    "y = [seq[i+max_len] for i in start_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fatak\\AppData\\Local\\Temp\\ipykernel_4336\\467088729.py:4: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  neg_item.append(random.sample({1,2,3},1))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "neg_item = []\n",
    "neg_item.append(random.sample({1,2,3},1))\n",
    "# torch.tensor([4], dtype = torch.long).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_item.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "class SimHashLayer:\n",
    "    def __init__(self, num_bits):\n",
    "        \"\"\"\n",
    "        初始化函数，用于设置哈希位数\n",
    "        :param num_bits: SimHash结果的位数，也就是最终哈希值的长度\n",
    "        \"\"\"\n",
    "        self.num_bits = num_bits\n",
    "        self.weight_matrix = torch.randn(32,64, self.num_bits)\n",
    "\n",
    "    def hash_emb_layer(self, inputs):\n",
    "        \"\"\"\n",
    "        实现类似SimHash的功能\n",
    "        :param inputs: 输入的嵌入向量，形状为[B,..., D]，这里B表示batch size，D表示嵌入维度\n",
    "        :return: 生成的SimHash值，形状为[B,..., num_bits]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    "        # 通过矩阵乘法将输入嵌入向量与权重矩阵相乘，来得到每个样本在每个哈希位上的初步得分\n",
    "        # 输出形状变为 [B,..., num_bits]\n",
    "        hash_scores = torch.bmm(inputs, self.weight_matrix)\n",
    "\n",
    "        # 通过符号函数将得分转换为 -1 或者 1，来确定最终每个哈希位上的值\n",
    "        hash_values = torch.sign(hash_scores)\n",
    "\n",
    "        # 将 -1 转换为 0，使得最终的哈希值每个元素都是 0 或者 1\n",
    "        hash_values = (hash_values + 1) / 2\n",
    "        hash_values = hash_values.type(torch.int64)\n",
    "\n",
    "        return hash_values\n",
    "    \n",
    "simhash = SimHashLayer(16)\n",
    "tensor1 = torch.randn((32,100,64))\n",
    "simhash.hash_emb_layer(tensor1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 100])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(low=0,high=2,size=(256,100,16))\n",
    "y = torch.randint(low=0,high=2,size=(256,1,16))\n",
    "match_buckets = torch.bitwise_xor(x,y)\n",
    "torch.sum(match_buckets, dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2**i  for i in range(8)],dtype=torch.uint8,requires_grad = False)\n",
    "y = torch.randint(low=0,high=2,size=(256,1,8),dtype=torch.uint8)\n",
    "z = torch.randint(low=0,high=2,size=(256,100,8),dtype=torch.uint8)\n",
    "t1 = torch.matmul(y,x)\n",
    "t2 = torch.matmul(z,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 100])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  4,  7,  9, 12, 13, 14, 15, 17, 19, 20, 24, 26, 27, 29, 30, 31,\n",
       "        34, 36, 39, 44, 45, 46, 49, 51, 52, 53, 55,  3,  4, 10, 11, 12, 15, 16,\n",
       "        17, 18, 21, 22, 25, 26, 27, 28, 29, 30, 31, 33, 35, 41, 44, 48, 53, 55,\n",
       "         9, 12, 13, 14, 17, 21, 23, 24, 25, 27, 31, 33, 34, 35, 37, 39, 41, 43,\n",
       "        44, 45, 48, 49, 50, 54, 55,  0,  2,  5,  7,  8, 10, 12, 14, 15, 16, 17,\n",
       "        19, 20, 22, 24, 25, 29, 33, 34, 35, 40, 41, 43, 45, 50, 51, 52])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "bucket_match = torch.randint(low=0,high=2,size=(4,6,10))\n",
    "y = torch.nonzero(bucket_match.flatten(start_dim=1),as_tuple=True)\n",
    "collide_mask = (bucket_match == 0).float()\n",
    "collide_mask.sum(dim=-1).shape\n",
    "y[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6400])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SDIM(BaseModel):\n",
    "    def __init__(self, \n",
    "                 feature_map, \n",
    "                 model_id=\"SDIM\", \n",
    "                 gpu=-1, \n",
    "                 dnn_hidden_units=[512, 128, 64],\n",
    "                 dnn_activations=\"ReLU\",\n",
    "                 attention_dim=64,\n",
    "                 use_qkvo=True,\n",
    "                 num_heads=1,\n",
    "                 use_scale=True,\n",
    "                 attention_dropout=0,\n",
    "                 reuse_hash=True,\n",
    "                 num_hashes=1,\n",
    "                 hash_bits=4,\n",
    "                 learning_rate=1e-3,\n",
    "                 embedding_dim=10, \n",
    "                 net_dropout=0, \n",
    "                 batch_norm=False, \n",
    "                 short_target_field=[(\"item_id\", \"cate_id\")],\n",
    "                 short_sequence_field=[(\"click_history\", \"cate_history\")],\n",
    "                 long_target_field=[(\"item_id\", \"cate_id\")],\n",
    "                 long_sequence_field=[(\"click_history\", \"cate_history\")],\n",
    "                 embedding_regularizer=None,\n",
    "                 net_regularizer=None,\n",
    "                 **kwargs):\n",
    "        super(SDIM, self).__init__(feature_map,\n",
    "                                   model_id=model_id, \n",
    "                                   gpu=gpu,\n",
    "                                   embedding_regularizer=embedding_regularizer, \n",
    "                                   net_regularizer=net_regularizer,\n",
    "                                   **kwargs)\n",
    "        if type(short_target_field) != list:\n",
    "            short_target_field = [short_target_field]\n",
    "        if type(short_sequence_field) != list:\n",
    "            short_sequence_field = [short_sequence_field]\n",
    "        if type(long_target_field) != list:\n",
    "            long_target_field = [long_target_field]\n",
    "        if type(long_sequence_field) != list:\n",
    "            long_sequence_field = [long_sequence_field]           \n",
    "        self.short_target_field = short_target_field\n",
    "        self.short_sequence_field = short_sequence_field\n",
    "        self.long_target_field = long_target_field\n",
    "        self.long_sequence_field = long_sequence_field\n",
    "        assert len(self.short_target_field) == len(self.short_sequence_field) \\\n",
    "               and len(self.long_target_field) == len(self.long_sequence_field), \\\n",
    "               \"Config error: target_field mismatches with sequence_field.\"\n",
    "        self.feature_map = feature_map\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.reuse_hash = reuse_hash\n",
    "        self.num_hashes = num_hashes\n",
    "        self.hash_bits = hash_bits\n",
    "        self.powers_of_two = nn.Parameter(torch.tensor([2.0 ** i for i in range(hash_bits)]), \n",
    "                                          requires_grad=False)\n",
    "        self.embedding_layer = FeatureEmbeddingDict(feature_map, embedding_dim)\n",
    "        self.short_attention = nn.ModuleList()\n",
    "        for target_field in self.short_target_field:\n",
    "            if type(target_field) == tuple:\n",
    "                input_dim = embedding_dim * len(target_field)\n",
    "            else:\n",
    "                input_dim = embedding_dim\n",
    "            self.short_attention.append(MultiHeadTargetAttention(input_dim,\n",
    "                                                                 attention_dim,\n",
    "                                                                 num_heads,\n",
    "                                                                 attention_dropout,\n",
    "                                                                 use_scale,\n",
    "                                                                 use_qkvo))\n",
    "        self.random_rotations = nn.ParameterList()\n",
    "        for target_field in self.long_target_field:\n",
    "            if type(target_field) == tuple:\n",
    "                input_dim = embedding_dim * len(target_field)\n",
    "            else:\n",
    "                input_dim = embedding_dim\n",
    "            self.random_rotations.append(nn.Parameter(torch.randn(input_dim, self.num_hashes, \n",
    "                                                      self.hash_bits), requires_grad=False))\n",
    "        self.dnn = MLP_Block(input_dim=feature_map.sum_emb_out_dim(),\n",
    "                             output_dim=1,\n",
    "                             hidden_units=dnn_hidden_units,\n",
    "                             hidden_activations=dnn_activations,\n",
    "                             output_activation=self.output_activation, \n",
    "                             dropout_rates=net_dropout,\n",
    "                             batch_norm=batch_norm)\n",
    "        self.compile(kwargs[\"optimizer\"], kwargs[\"loss\"], learning_rate)\n",
    "        self.reset_parameters()\n",
    "        self.model_to_device()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        X = self.get_inputs(inputs)\n",
    "        feature_emb_dict = self.embedding_layer(X)\n",
    "        # short interest attention\n",
    "        for idx, (target_field, sequence_field) in enumerate(zip(self.short_target_field, \n",
    "                                                                 self.short_sequence_field)):\n",
    "            target_emb = self.concat_embedding(target_field, feature_emb_dict)\n",
    "            sequence_emb = self.concat_embedding(sequence_field, feature_emb_dict)\n",
    "            seq_field = list(flatten([sequence_field]))[0] # flatten nested list to pick the first field\n",
    "            mask = X[seq_field].long() != 0 # padding_idx = 0 required in input data\n",
    "            short_interest_emb = self.short_attention[idx](target_emb, sequence_emb, mask)\n",
    "            for field, field_emb in zip(list(flatten([sequence_field])),\n",
    "                                        short_interest_emb.split(self.embedding_dim, dim=-1)):\n",
    "                feature_emb_dict[field] = field_emb\n",
    "        # long interest attention\n",
    "        for idx, (target_field, sequence_field) in enumerate(zip(self.long_target_field, \n",
    "                                                                 self.long_sequence_field)):\n",
    "            target_emb = self.concat_embedding(target_field, feature_emb_dict)\n",
    "            sequence_emb = self.concat_embedding(sequence_field, feature_emb_dict)\n",
    "            long_interest_emb = self.lsh_attentioin(self.random_rotations[idx], \n",
    "                                                    target_emb, sequence_emb)\n",
    "            for field, field_emb in zip(list(flatten([sequence_field])),\n",
    "                                        long_interest_emb.split(self.embedding_dim, dim=-1)):\n",
    "                feature_emb_dict[field] = field_emb\n",
    "        feature_emb = self.embedding_layer.dict2tensor(feature_emb_dict, dynamic_emb_dim=True)\n",
    "        y_pred = self.dnn(feature_emb.flatten(start_dim=1))\n",
    "        return_dict = {\"y_pred\": y_pred}\n",
    "        return return_dict\n",
    "\n",
    "    def concat_embedding(self, field, feature_emb_dict):\n",
    "        if type(field) == tuple:\n",
    "            emb_list = [feature_emb_dict[f] for f in field]\n",
    "            return torch.cat(emb_list, dim=-1)\n",
    "        else:\n",
    "            return feature_emb_dict[field]\n",
    "    # 彻底理解了，叹为观止\n",
    "    def lsh_attentioin(self, random_rotations, target_item, history_sequence):\n",
    "        if not self.reuse_hash:\n",
    "            random_rotations = torch.randn(target_item.size(1), self.num_hashes, \n",
    "                                           self.hash_bits, device=target_item.device)\n",
    "        # num_hashes表示的是分桶的数量，hash_bits表示的是一个通的bit数量\n",
    "        # 我们要得到的是对分桶内做求和，分桶间做avgpooling\n",
    "        # 分桶聚合的对象是L序列，也就是要将BLD-》BD\n",
    "        target_bucket = self.lsh_hash(history_sequence, random_rotations)\n",
    "        sequence_bucket = self.lsh_hash(target_item.unsqueeze(1), random_rotations)\n",
    "        # 之所以转置是因为BLD，计算碰撞数量，collide_mask表示的序列中标识了发生碰撞的位置\n",
    "        bucket_match = (sequence_bucket - target_bucket).permute(2, 0, 1) # num_hashes x B x seq_len\n",
    "        collide_mask = (bucket_match == 0).float()\n",
    "        # torch.nonzero将为1的位置提取出来，得到collide_index，collide_index表示的是序列中发生碰撞的index，\n",
    "        # 且重复了num_hashes次\n",
    "        hash_index, collide_index = torch.nonzero(collide_mask.flatten(start_dim=1), as_tuple=True)\n",
    "        # .sum表示的是每一个batch每一个hash分桶中user序列与target_item碰撞的数量，\n",
    "        # .cumsum表示的是前缀和，表示的是第i个batch的位置，offsets表示分袋的数量，且指示的是索引位置\n",
    "        offsets = collide_mask.sum(dim=-1).long().flatten().cumsum(dim=0)\n",
    "        # 根据collide_index以及offsets，从history_sequence拼接向量并求和\n",
    "        attn_out = F.embedding_bag(collide_index, history_sequence.view(-1, target_item.size(1)), \n",
    "                                   offsets, mode='sum') # (num_hashes x B) x d\n",
    "        attn_out = attn_out.view(self.num_hashes, -1, target_item.size(1)).mean(dim=0) # B x d\n",
    "        return attn_out\n",
    "        \n",
    "    def lsh_hash(self, vecs, random_rotations):\n",
    "        \"\"\" See the tensorflow-lsh-functions for reference:\n",
    "            https://github.com/brc7/tensorflow-lsh-functions/blob/main/lsh_functions.py\n",
    "            \n",
    "            Input: vecs, with shape B x seq_len x d\n",
    "            Output: hash_bucket, with shape B x seq_len x num_hashes\n",
    "        \"\"\"\n",
    "        rotated_vecs = torch.einsum(\"bld,dht->blht\", vecs, random_rotations) # B x seq_len x num_hashes x hash_bits\n",
    "        hash_code = torch.relu(torch.sign(rotated_vecs))\n",
    "        hash_bucket = torch.matmul(hash_code, self.powers_of_two.unsqueeze(-1)).squeeze(-1)\n",
    "        return hash_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\fatak\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\D\\code\\longSeqRec\\wandb\\run-20241219_200503-k9ni38i4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/1434615370-renmin-university-of-china/my-awesome-project/runs/k9ni38i4' target=\"_blank\">stoic-bee-1</a></strong> to <a href='https://wandb.ai/1434615370-renmin-university-of-china/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/1434615370-renmin-university-of-china/my-awesome-project' target=\"_blank\">https://wandb.ai/1434615370-renmin-university-of-china/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/1434615370-renmin-university-of-china/my-awesome-project/runs/k9ni38i4' target=\"_blank\">https://wandb.ai/1434615370-renmin-university-of-china/my-awesome-project/runs/k9ni38i4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▄▃▆▅▆▇█</td></tr><tr><td>loss</td><td>▅█▆▃▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.95985</td></tr><tr><td>loss</td><td>0.11925</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-bee-1</strong> at: <a href='https://wandb.ai/1434615370-renmin-university-of-china/my-awesome-project/runs/k9ni38i4' target=\"_blank\">https://wandb.ai/1434615370-renmin-university-of-china/my-awesome-project/runs/k9ni38i4</a><br/> View project at: <a href='https://wandb.ai/1434615370-renmin-university-of-china/my-awesome-project' target=\"_blank\">https://wandb.ai/1434615370-renmin-university-of-china/my-awesome-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241219_200503-k9ni38i4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "\n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longSeq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
